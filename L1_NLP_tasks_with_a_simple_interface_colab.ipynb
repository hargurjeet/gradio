{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hargurjeet/gradio_experiments/blob/master/L1_NLP_tasks_with_a_simple_interface_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88535ead",
      "metadata": {
        "id": "88535ead"
      },
      "source": [
        "# L1: NLP tasks with a simple interface üóûÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6faa43ba",
      "metadata": {
        "id": "6faa43ba"
      },
      "source": [
        "Load your HF API key and relevant Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d2698081-4deb-436a-a821-8ea48bdd6e6a",
      "metadata": {
        "id": "d2698081-4deb-436a-a821-8ea48bdd6e6a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "from IPython.display import Image, display, HTML\n",
        "from PIL import Image\n",
        "import base64\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
        "# hf_api_key = os.environ['HF_API_KEY']\n",
        "YOUR_HUGGING_FACE_TOKEN = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97a06f9",
      "metadata": {
        "id": "a97a06f9"
      },
      "source": [
        "## Building a text summarization app"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4794502",
      "metadata": {
        "id": "d4794502"
      },
      "source": [
        "Here we are using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for the `shleifer/distilbart-cnn-12-6`, a 306M parameter distilled model from `facebook/bart-large-cnn`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "iKsu1ibgrFgs"
      },
      "id": "iKsu1ibgrFgs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5fc667c5",
      "metadata": {
        "id": "5fc667c5"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "get_completion = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", use_auth_token=YOUR_HUGGING_FACE_TOKEN)\n",
        "\n",
        "def summarize(input):\n",
        "    output = get_completion(input)\n",
        "    return output[0]['summary_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01892d1a",
      "metadata": {
        "id": "01892d1a"
      },
      "source": [
        "### How about running it locally?\n",
        "The code would look very similar if you were running it locally instead of from an API. The same is true for all the models in the rest of the course, make sure to check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page\n",
        "\n",
        "```py\n",
        "from transformers import pipeline\n",
        "\n",
        "get_completion = pipeline(\"summarization\", model=\"shleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "def summarize(input):\n",
        "    output = get_completion(input)\n",
        "    return output[0]['summary_text']\n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c2f0fc58-91d6-48f2-a014-052192586be8",
      "metadata": {
        "id": "c2f0fc58-91d6-48f2-a014-052192586be8",
        "outputId": "219d8072-a1d2-4290-b75c-878702425bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2730f4b01e38>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         after the Millau Viaduct.''')\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         if (\n\u001b[1;32m    167\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             )\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# All unused kwargs must be model kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0;31m# 2. Set generation parameters if not already defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munused_model_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1250\u001b[0m                 \u001b[0;34mf\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                 \u001b[0;34m\" generate arguments will also show up in this list)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['use_auth_token'] (note: typos in the generate arguments will also show up in this list)"
          ]
        }
      ],
      "source": [
        "text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n",
        "        as an 81-storey building, and the tallest structure in Paris.\n",
        "        Its base is square, measuring 125 metres (410 ft) on each side.\n",
        "        During its construction, the Eiffel Tower surpassed the Washington\n",
        "        Monument to become the tallest man-made structure in the world,\n",
        "        a title it held for 41 years until the Chrysler Building\n",
        "        in New York City was finished in 1930. It was the first structure\n",
        "        to reach a height of 300 metres. Due to the addition of a broadcasting\n",
        "        aerial at the top of the tower in 1957, it is now taller than the\n",
        "        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the\n",
        "        Eiffel Tower is the second tallest free-standing structure in France\n",
        "        after the Millau Viaduct.''')\n",
        "\n",
        "get_completion(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer, pipeline\n",
        "\n",
        "YOUR_HUGGING_FACE_TOKEN = \"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\"\n",
        "\n",
        "# Load the model and tokenizer using the authentication token\n",
        "model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-12-6\", use_auth_token=YOUR_HUGGING_FACE_TOKEN)\n",
        "tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\", use_auth_token=YOUR_HUGGING_FACE_TOKEN)\n",
        "\n",
        "# Create the summarization pipeline with the loaded model and tokenizer\n",
        "get_completion = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def summarize(input):\n",
        "    output = get_completion(input)\n",
        "    return output[0]['summary_text']\n",
        "\n",
        "# Test\n",
        "text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n",
        "        as an 81-storey building, and the tallest structure in Paris.\n",
        "        Its base is square, measuring 125 metres (410 ft) on each side.\n",
        "        During its construction, the Eiffel Tower surpassed the Washington\n",
        "        Monument to become the tallest man-made structure in the world,\n",
        "        a title it held for 41 years until the Chrysler Building\n",
        "        in New York City was finished in 1930. It was the first structure\n",
        "        to reach a height of 300 metres. Due to the addition of a broadcasting\n",
        "        aerial at the top of the tower in 1957, it is now taller than the\n",
        "        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the\n",
        "        Eiffel Tower is the second tallest free-standing structure in France\n",
        "        after the Millau Viaduct.''')\n",
        "\n",
        "print(summarize(text))\n"
      ],
      "metadata": {
        "id": "itx1JziLt3ga",
        "outputId": "988f812d-d29c-42ef-d01e-43146302ad1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "itx1JziLt3ga",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2363: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1727: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building . It is the tallest structure in Paris and the second tallest free-standing structure in France after the Millau Viaduct . It was the first structure in the world to reach a height of 300 metres .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f144593f",
      "metadata": {
        "id": "f144593f"
      },
      "source": [
        "### Getting started with Gradio `gr.Interface`\n",
        "\n",
        "#### How about running it locally?\n",
        "The code would look very similar if you were running it locally.  Simply remove all the paramters in the launch method\n",
        "\n",
        "```py\n",
        "demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb11460",
      "metadata": {
        "id": "3eb11460",
        "outputId": "d9f31925-56ed-4acb-a10f-82955ab1169a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "def summarize(input):\n",
        "    output = get_completion(input)\n",
        "    return output[0]['summary_text']\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=summarize, inputs=\"text\", outputs=\"text\")\n",
        "# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b310770",
      "metadata": {
        "id": "9b310770"
      },
      "source": [
        "You can add `demo.launch(share=True)` to create a public link to share with your team or friends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60684b55-c7ae-4c9e-88ea-bbc2e702ecdb",
      "metadata": {
        "id": "60684b55-c7ae-4c9e-88ea-bbc2e702ecdb",
        "outputId": "67502a5a-c43c-42a8-e50f-598f82fc11b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def summarize(input):\n",
        "    output = get_completion(input)\n",
        "    return output[0]['summary_text']\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=summarize,\n",
        "                    inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
        "                    outputs=[gr.Textbox(label=\"Result\", lines=3)],\n",
        "                    title=\"Text summarization with distilbart-cnn\",\n",
        "                    description=\"Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!\"\n",
        "                   )\n",
        "# demo.launch(share=True, server_port=int(os.environ['PORT2']))\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b300d17",
      "metadata": {
        "id": "4b300d17"
      },
      "source": [
        "## Building a Named Entity Recognition app"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d1043f",
      "metadata": {
        "id": "c0d1043f"
      },
      "source": [
        "We are using this [Inference Endpoint](https://huggingface.co/inference-endpoints) for `dslim/bert-base-NER`, a 108M parameter fine-tuned BART model on the NER task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f663dcbb",
      "metadata": {
        "id": "f663dcbb"
      },
      "source": [
        "### How about running it locally?\n",
        "\n",
        "```py\n",
        "from transformers import pipeline\n",
        "\n",
        "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
        "\n",
        "def ner(input):\n",
        "    output = get_completion(input)\n",
        "    return {\"text\": input, \"entities\": output}\n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad465622",
      "metadata": {
        "id": "ad465622",
        "outputId": "fa5cf8ce-aa62-4fad-9a6e-6df655225424",
        "colab": {
          "referenced_widgets": [
            "1607db3a41db498e9705af6523e7e8c6",
            "bfaaacbda41c406b80a4ec22411088aa",
            "81e72f0afe434d86b0ab26c4ab53fbd6",
            "5a5146042511472fbe77d32e49114ee2",
            "f0c2823c9cc44bcf8bc9823e4e9e8e85",
            "e2d147bb15284989995952b22bf65092"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0279998779296875,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading config.json",
              "rate": null,
              "total": 829,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1607db3a41db498e9705af6523e7e8c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.017642498016357422,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading pytorch_model.bin",
              "rate": null,
              "total": 433316646,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfaaacbda41c406b80a4ec22411088aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/413M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.029843568801879883,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading tokenizer_config.json",
              "rate": null,
              "total": 59,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81e72f0afe434d86b0ab26c4ab53fbd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.08349061012268066,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading vocab.txt",
              "rate": null,
              "total": 213450,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a5146042511472fbe77d32e49114ee2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.02702045440673828,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading added_tokens.json",
              "rate": null,
              "total": 2,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0c2823c9cc44bcf8bc9823e4e9e8e85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.04295206069946289,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading special_tokens_map.json",
              "rate": null,
              "total": 112,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2d147bb15284989995952b22bf65092",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
        "\n",
        "def ner(input):\n",
        "    output = get_completion(input)\n",
        "    return {\"text\": input, \"entities\": output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5b47c1",
      "metadata": {
        "id": "2c5b47c1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db4a922-b300-4dbc-8768-955b6a18dce4",
      "metadata": {
        "id": "0db4a922-b300-4dbc-8768-955b6a18dce4",
        "outputId": "b8e97e91-13d6-43b2-d3a2-88b7b4df4bb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'B-PER',\n",
              "  'score': 0.9990625,\n",
              "  'index': 4,\n",
              "  'word': 'Andrew',\n",
              "  'start': 11,\n",
              "  'end': 17},\n",
              " {'entity': 'B-ORG',\n",
              "  'score': 0.9927857,\n",
              "  'index': 10,\n",
              "  'word': 'Deep',\n",
              "  'start': 32,\n",
              "  'end': 36},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': 0.99677867,\n",
              "  'index': 11,\n",
              "  'word': '##L',\n",
              "  'start': 36,\n",
              "  'end': 37},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': 0.9954496,\n",
              "  'index': 12,\n",
              "  'word': '##ear',\n",
              "  'start': 37,\n",
              "  'end': 40},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': 0.9959293,\n",
              "  'index': 13,\n",
              "  'word': '##ning',\n",
              "  'start': 40,\n",
              "  'end': 44},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': 0.89174646,\n",
              "  'index': 14,\n",
              "  'word': '##A',\n",
              "  'start': 44,\n",
              "  'end': 45},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': 0.5036118,\n",
              "  'index': 15,\n",
              "  'word': '##I',\n",
              "  'start': 45,\n",
              "  'end': 46},\n",
              " {'entity': 'B-LOC',\n",
              "  'score': 0.99969244,\n",
              "  'index': 20,\n",
              "  'word': 'California',\n",
              "  'start': 61,\n",
              "  'end': 71}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint\n",
        "text = \"My name is Andrew, I'm building DeepLearningAI and I live in California\"\n",
        "get_completion(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c21254-128d-446c-b6dd-e30af26d436d",
      "metadata": {
        "id": "e5c21254-128d-446c-b6dd-e30af26d436d",
        "outputId": "7bb60a23-7004-4997-e55e-3a904fad5dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ner(input):\n",
        "    output = get_completion(input)\n",
        "    return {\"text\": input, \"entities\": output}\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=ner,\n",
        "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
        "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
        "                    title=\"NER with dslim/bert-base-NER\",\n",
        "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
        "                    allow_flagging=\"never\",\n",
        "                    #Here we introduce a new tag, examples, easy to use examples for your application\n",
        "                    examples=[\"My name is Andrew and I live in California\", \"My name is Poli and work at HuggingFace\"])\n",
        "# demo.launch(share=True, server_port=int(os.environ['PORT3']))\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f16ad4",
      "metadata": {
        "id": "60f16ad4"
      },
      "source": [
        "### Adding a helper function to merge tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc278e9-87b4-420b-89e9-7120dc4be754",
      "metadata": {
        "id": "4dc278e9-87b4-420b-89e9-7120dc4be754",
        "outputId": "01728d4c-72de-4f46-b091-0c580f83b019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def merge_tokens(tokens):\n",
        "    merged_tokens = []\n",
        "    for token in tokens:\n",
        "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
        "            # If current token continues the entity of the last one, merge them\n",
        "            last_token = merged_tokens[-1]\n",
        "            last_token['word'] += token['word'].replace('##', '')\n",
        "            last_token['end'] = token['end']\n",
        "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
        "        else:\n",
        "            # Otherwise, add the token to the list\n",
        "            merged_tokens.append(token)\n",
        "\n",
        "    return merged_tokens\n",
        "\n",
        "def ner(input):\n",
        "    output = get_completion(input)\n",
        "    merged_tokens = merge_tokens(output)\n",
        "    return {\"text\": input, \"entities\": merged_tokens}\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=ner,\n",
        "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
        "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
        "                    title=\"NER with dslim/bert-base-NER\",\n",
        "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
        "                    allow_flagging=\"never\",\n",
        "                    examples=[\"My name is Andrew, I'm building DeeplearningAI and I live in California\", \"My name is Poli, I live in Vienna and work at HuggingFace\"])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cccdb9b-0c3a-406e-95bc-106705aeb010",
      "metadata": {
        "id": "3cccdb9b-0c3a-406e-95bc-106705aeb010",
        "outputId": "b745fed7-7a1f-43b7-8439-6838fc5becac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "gr.close_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c428938-aec5-46f6-9315-af93a7a8abb5",
      "metadata": {
        "id": "5c428938-aec5-46f6-9315-af93a7a8abb5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca47b403-f187-4d7f-929e-a1c09012b741",
      "metadata": {
        "id": "ca47b403-f187-4d7f-929e-a1c09012b741"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33eac55-3520-443a-a7fd-a2a6c8768ec7",
      "metadata": {
        "id": "d33eac55-3520-443a-a7fd-a2a6c8768ec7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37d1a9d-9f3a-4f38-bdbc-08b97d17a72c",
      "metadata": {
        "id": "a37d1a9d-9f3a-4f38-bdbc-08b97d17a72c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c468c2f8-5123-4121-962c-326befeb7789",
      "metadata": {
        "id": "c468c2f8-5123-4121-962c-326befeb7789"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eda551f-ca2e-4a8c-8fc0-f921ba1eb172",
      "metadata": {
        "id": "5eda551f-ca2e-4a8c-8fc0-f921ba1eb172"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af69688-df59-45a6-be9f-1e66cae7ac1f",
      "metadata": {
        "id": "2af69688-df59-45a6-be9f-1e66cae7ac1f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b808da8-a99e-4fd0-b984-32640d90c13f",
      "metadata": {
        "id": "3b808da8-a99e-4fd0-b984-32640d90c13f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc373ed3-ce8d-402e-bbda-837e9478c2c5",
      "metadata": {
        "id": "bc373ed3-ce8d-402e-bbda-837e9478c2c5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31eba1e9-e994-4480-9076-99a7b71d3211",
      "metadata": {
        "id": "31eba1e9-e994-4480-9076-99a7b71d3211"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "189844a4-1fcc-4ccc-bcff-9c9d3ec6759c",
      "metadata": {
        "id": "189844a4-1fcc-4ccc-bcff-9c9d3ec6759c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bce126b-c54e-4fec-8d4b-462317149f1e",
      "metadata": {
        "id": "2bce126b-c54e-4fec-8d4b-462317149f1e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6833b2ac-f605-4566-b84a-c0c2bc529bd7",
      "metadata": {
        "id": "6833b2ac-f605-4566-b84a-c0c2bc529bd7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef1d07f-ac8b-4f38-9c6c-ecdd5942ffb6",
      "metadata": {
        "id": "2ef1d07f-ac8b-4f38-9c6c-ecdd5942ffb6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5c6dd3-bc15-4139-bee8-d7291c20a1e8",
      "metadata": {
        "id": "ab5c6dd3-bc15-4139-bee8-d7291c20a1e8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ddfa37-da43-40be-a3da-05aa3d2ee9c8",
      "metadata": {
        "id": "62ddfa37-da43-40be-a3da-05aa3d2ee9c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50dafb5-50bf-4b8f-9eec-84682ae339e9",
      "metadata": {
        "id": "e50dafb5-50bf-4b8f-9eec-84682ae339e9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13755d46-db04-435f-bbe8-c6b6df9f46fc",
      "metadata": {
        "id": "13755d46-db04-435f-bbe8-c6b6df9f46fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ea1054-75c1-42c5-8fe3-1f5a1eb9b8fc",
      "metadata": {
        "id": "63ea1054-75c1-42c5-8fe3-1f5a1eb9b8fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af00aba6-d0e7-49be-816c-0fa4670737fd",
      "metadata": {
        "id": "af00aba6-d0e7-49be-816c-0fa4670737fd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}