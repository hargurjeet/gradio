{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88535ead",
   "metadata": {},
   "source": [
    "# L1: NLP tasks with a simple interface 🗞️"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6faa43ba",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2698081-4deb-436a-a821-8ea48bdd6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "import base64 \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106ab02-f248-4c03-9dd8-b1991db7f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "\n",
    "#Summarization endpoint\n",
    "def get_completion(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']): \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL, headers=headers,\n",
    "                                data=json.dumps(data)\n",
    "                               )\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a97a06f9",
   "metadata": {},
   "source": [
    "## Building a text summarization app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4794502",
   "metadata": {},
   "source": [
    "Here we are using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for the `shleifer/distilbart-cnn-12-6`, a 306M parameter distilled model from `facebook/bart-large-cnn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc667c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'shleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'shleifer/distilbart-cnn-12-6' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\614654958\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\configuration_utils.py:616\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    615\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 616\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_path(\n\u001b[0;32m    617\u001b[0m         config_file,\n\u001b[0;32m    618\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    619\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    620\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    621\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    622\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    623\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    624\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    627\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\614654958\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\utils\\hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    283\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m    285\u001b[0m         url_or_filename,\n\u001b[0;32m    286\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    287\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    288\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    289\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    290\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    291\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    292\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    293\u001b[0m     )\n\u001b[0;32m    294\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    295\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\614654958\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\utils\\hub.py:494\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou specified use_auth_token=True, but a huggingface token was not found.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    495\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mauthorization\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBearer \u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mOSError\u001b[0m: You specified use_auth_token=True, but a huggingface token was not found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\614654958\\Downloads\\L1_NLP_tasks_with_a_simple_interface.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/614654958/Downloads/L1_NLP_tasks_with_a_simple_interface.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/614654958/Downloads/L1_NLP_tasks_with_a_simple_interface.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_completion \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msummarization\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mshleifer/distilbart-cnn-12-6\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\614654958\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\__init__.py:574\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(config, _from_pipeline\u001b[39m=\u001b[39mtask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[39melif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 574\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(model, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[0;32m    576\u001b[0m custom_tasks \u001b[39m=\u001b[39m {}\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mgetattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mcustom_pipelines\u001b[39m\u001b[39m\"\u001b[39m, {})) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\614654958\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:725\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    724\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 725\u001b[0m config_dict, _ \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    727\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Users\\614654958\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\configuration_utils.py:561\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    560\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    563\u001b[0m \u001b[39m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfiguration_files\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n",
      "File \u001b[1;32mc:\\Users\\614654958\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\configuration_utils.py:656\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    650\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this model, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m the cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    653\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m library in offline mode at \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    654\u001b[0m     )\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m--> 656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    657\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load config for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    658\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    661\u001b[0m     )\n\u001b[0;32m    663\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    664\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[0;32m    665\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load config for 'shleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'shleifer/distilbart-cnn-12-6' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"summarization\", model=\"shleifer/distilbart-cnn-12-6\", use_auth_token=True)\n",
    "\n",
    "# def summarize(input):\n",
    "#     output = get_completion(input)\n",
    "#     return output[0]['summary_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01892d1a",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "The code would look very similar if you were running it locally instead of from an API. The same is true for all the models in the rest of the course, make sure to check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"summarization\", model=\"shleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0fc58-91d6-48f2-a014-052192586be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n",
    "        as an 81-storey building, and the tallest structure in Paris. \n",
    "        Its base is square, measuring 125 metres (410 ft) on each side. \n",
    "        During its construction, the Eiffel Tower surpassed the Washington \n",
    "        Monument to become the tallest man-made structure in the world,\n",
    "        a title it held for 41 years until the Chrysler Building\n",
    "        in New York City was finished in 1930. It was the first structure \n",
    "        to reach a height of 300 metres. Due to the addition of a broadcasting \n",
    "        aerial at the top of the tower in 1957, it is now taller than the \n",
    "        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the \n",
    "        Eiffel Tower is the second tallest free-standing structure in France \n",
    "        after the Millau Viaduct.''')\n",
    "\n",
    "get_completion(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f144593f",
   "metadata": {},
   "source": [
    "### Getting started with Gradio `gr.Interface` \n",
    "\n",
    "#### How about running it locally?\n",
    "The code would look very similar if you were running it locally.  Simply remove all the paramters in the launch method\n",
    "\n",
    "```py\n",
    "demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb11460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b310770",
   "metadata": {},
   "source": [
    "You can add `demo.launch(share=True)` to create a public link to share with your team or friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60684b55-c7ae-4c9e-88ea-bbc2e702ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, \n",
    "                    inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
    "                    outputs=[gr.Textbox(label=\"Result\", lines=3)],\n",
    "                    title=\"Text summarization with distilbart-cnn\",\n",
    "                    description=\"Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!\"\n",
    "                   )\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b300d17",
   "metadata": {},
   "source": [
    "## Building a Named Entity Recognition app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d1043f",
   "metadata": {},
   "source": [
    "We are using this [Inference Endpoint](https://huggingface.co/inference-endpoints) for `dslim/bert-base-NER`, a 108M parameter fine-tuned BART model on the NER task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f663dcbb",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4a922-b300-4dbc-8768-955b6a18dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint\n",
    "text = \"My name is Andrew, I'm building DeepLearningAI and I live in California\"\n",
    "get_completion(text, parameters=None, ENDPOINT_URL= API_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c21254-128d-446c-b6dd-e30af26d436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    #Here we introduce a new tag, examples, easy to use examples for your application\n",
    "                    examples=[\"My name is Andrew and I live in California\", \"My name is Poli and work at HuggingFace\"])\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f16ad4",
   "metadata": {},
   "source": [
    "### Adding a helper function to merge tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc278e9-87b4-420b-89e9-7120dc4be754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            # If current token continues the entity of the last one, merge them\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            # Otherwise, add the token to the list\n",
    "            merged_tokens.append(token)\n",
    "\n",
    "    return merged_tokens\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return {\"text\": input, \"entities\": merged_tokens}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"My name is Andrew, I'm building DeeplearningAI and I live in California\", \"My name is Poli, I live in Vienna and work at HuggingFace\"])\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccdb9b-0c3a-406e-95bc-106705aeb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c428938-aec5-46f6-9315-af93a7a8abb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47b403-f187-4d7f-929e-a1c09012b741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33eac55-3520-443a-a7fd-a2a6c8768ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d1a9d-9f3a-4f38-bdbc-08b97d17a72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468c2f8-5123-4121-962c-326befeb7789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda551f-ca2e-4a8c-8fc0-f921ba1eb172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af69688-df59-45a6-be9f-1e66cae7ac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b808da8-a99e-4fd0-b984-32640d90c13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373ed3-ce8d-402e-bbda-837e9478c2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eba1e9-e994-4480-9076-99a7b71d3211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189844a4-1fcc-4ccc-bcff-9c9d3ec6759c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce126b-c54e-4fec-8d4b-462317149f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833b2ac-f605-4566-b84a-c0c2bc529bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1d07f-ac8b-4f38-9c6c-ecdd5942ffb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c6dd3-bc15-4139-bee8-d7291c20a1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddfa37-da43-40be-a3da-05aa3d2ee9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50dafb5-50bf-4b8f-9eec-84682ae339e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13755d46-db04-435f-bbe8-c6b6df9f46fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea1054-75c1-42c5-8fe3-1f5a1eb9b8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00aba6-d0e7-49be-816c-0fa4670737fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
